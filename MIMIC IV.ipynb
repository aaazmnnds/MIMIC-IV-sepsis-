{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d62fcb1-7c92-41d9-91b0-daaf1df38f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking files...\n",
      "✓ hosp/patients.csv.gz (0.00 GB)\n",
      "✓ hosp/admissions.csv.gz (0.02 GB)\n",
      "✓ hosp/diagnoses_icd.csv.gz (0.03 GB)\n",
      "✓ hosp/labevents.csv.gz (2.41 GB)\n",
      "✓ icu/icustays.csv.gz (0.00 GB)\n",
      "✓ icu/chartevents.csv.gz (3.26 GB)\n"
     ]
    }
   ],
   "source": [
    "## Please ask the data from the author\n",
    "\n",
    "## organizing the files\n",
    "\n",
    "import os\n",
    "\n",
    "MIMIC_PATH = '/Users/azmannads/Downloads/Dr. Zhang/mimic-iv-2.2/'\n",
    "\n",
    "files_to_check = [\n",
    "    'hosp/patients.csv.gz',\n",
    "    'hosp/admissions.csv.gz',\n",
    "    'hosp/diagnoses_icd.csv.gz',\n",
    "    'hosp/labevents.csv.gz',\n",
    "    'icu/icustays.csv.gz',\n",
    "    'icu/chartevents.csv.gz'\n",
    "]\n",
    "\n",
    "print(\"Checking files...\")\n",
    "for file in files_to_check:\n",
    "    full_path = os.path.join(MIMIC_PATH, file)\n",
    "    if os.path.exists(full_path):\n",
    "        size = os.path.getsize(full_path) / (1024**3)\n",
    "        print(f\"✓ {file} ({size:.2f} GB)\")\n",
    "    else:\n",
    "        print(f\"✗ {file} - NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac83888-d8d5-49c0-857b-846bb4987b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "   MIMIC Path: /Users/azmannads/Downloads/Dr. Zhang/mimic-iv-2.2/\n",
      "   Output Path: /Users/azmannads/Downloads/Dr. Zhang\n"
     ]
    }
   ],
   "source": [
    "## Run the extraction\n",
    "\n",
    "### Configuration\n",
    "\n",
    "# ============================================================================\n",
    "# MIMIC-IV SEPSIS COHORT EXTRACTION - COMPLETE SETUP\n",
    "# Author: Azman Nads\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MIMIC_PATH = '/Users/azmannads/Downloads/Dr. Zhang/mimic-iv-2.2/'\n",
    "OUTPUT_PATH = '/Users/azmannads/Downloads/Dr. Zhang'\n",
    "INTERMEDIATE_PATH = os.path.join(OUTPUT_PATH, 'intermediate')\n",
    "FINAL_PATH = os.path.join(OUTPUT_PATH, 'final')\n",
    "LOG_PATH = os.path.join(OUTPUT_PATH, 'logs')\n",
    "\n",
    "# Create directories\n",
    "Path(INTERMEDIATE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(FINAL_PATH).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOG_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "MIN_AGE = 18\n",
    "MISSING_THRESHOLD = 0.3\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Item IDs\n",
    "LAB_ITEMS = {\n",
    "    'ALT': [50861],\n",
    "    'AST': [50878],\n",
    "    'Lactate': [50813],\n",
    "    'Sodium': [50983, 50824],\n",
    "    'Chloride': [50902, 50806],\n",
    "    'Platelets': [51265]\n",
    "}\n",
    "\n",
    "VITAL_ITEMS = {\n",
    "    'HR': [220045],\n",
    "    'RR': [220210, 224690],\n",
    "}\n",
    "\n",
    "GCS_ITEMS = [220739, 223901, 223900, 198]\n",
    "\n",
    "SEPSIS_ICD10 = ['A40', 'A41', 'R65']\n",
    "SEPSIS_ICD9 = ['995', '785']\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"   MIMIC Path: {MIMIC_PATH}\")\n",
    "print(f\"   Output Path: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5707d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    STEP 1: EXTRACTING BASE COHORT                    \n",
      "======================================================================\n",
      "\n",
      "1. Loading MIMIC-IV tables...\n",
      "  Patients: 364,627\n",
      "  Admissions: 546,028\n",
      "  Diagnoses: 6,364,488\n",
      "\n",
      "2. Identifying sepsis patients...\n",
      "  Sepsis admissions: 32,065\n",
      "\n",
      "3. Filtering for surgical admissions...\n",
      "  Surgical sepsis: 852\n",
      "\n",
      "4. Adding demographics...\n",
      "  Adults (≥18): 852\n",
      "  Deaths: 59 (6.9%)\n",
      "\n",
      "Saved: /Users/azmannads/Downloads/Dr. Zhang/intermediate/cohort_base.csv\n",
      "   Cohort: 852 patients\n"
     ]
    }
   ],
   "source": [
    "## Extract base cohort\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: EXTRACT BASE COHORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 1: EXTRACTING BASE COHORT\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load core tables\n",
    "print(\"\\n1. Loading MIMIC-IV tables...\")\n",
    "patients = pd.read_csv(f'{MIMIC_PATH}hosp/patients.csv.gz', compression='gzip')\n",
    "print(f\"  Patients: {len(patients):,}\")\n",
    "\n",
    "admissions = pd.read_csv(f'{MIMIC_PATH}hosp/admissions.csv.gz', compression='gzip')\n",
    "print(f\"  Admissions: {len(admissions):,}\")\n",
    "\n",
    "diagnoses = pd.read_csv(f'{MIMIC_PATH}hosp/diagnoses_icd.csv.gz', compression='gzip')\n",
    "print(f\"  Diagnoses: {len(diagnoses):,}\")\n",
    "\n",
    "# Identify sepsis\n",
    "print(\"\\n2. Identifying sepsis patients...\")\n",
    "all_sepsis_codes = SEPSIS_ICD10 + SEPSIS_ICD9\n",
    "sepsis_diagnoses = diagnoses[\n",
    "    diagnoses['icd_code'].str.startswith(tuple(all_sepsis_codes), na=False)\n",
    "]\n",
    "sepsis_hadm_ids = sepsis_diagnoses['hadm_id'].unique()\n",
    "print(f\"  Sepsis admissions: {len(sepsis_hadm_ids):,}\")\n",
    "\n",
    "# Filter surgical\n",
    "print(\"\\n3. Filtering for surgical admissions...\")\n",
    "surgical_sepsis = admissions[\n",
    "    (admissions['hadm_id'].isin(sepsis_hadm_ids)) &\n",
    "    (\n",
    "        (admissions['admission_type'].str.contains('SURGICAL', case=False, na=False)) |\n",
    "        (admissions['admission_location'].str.contains('SURGERY', case=False, na=False))\n",
    "    )\n",
    "]\n",
    "print(f\"  Surgical sepsis: {len(surgical_sepsis):,}\")\n",
    "\n",
    "# Add demographics\n",
    "print(\"\\n4. Adding demographics...\")\n",
    "cohort = surgical_sepsis.merge(patients, on='subject_id', how='left')\n",
    "\n",
    "# Calculate age\n",
    "cohort['admittime'] = pd.to_datetime(cohort['admittime'])\n",
    "cohort['age'] = cohort['admittime'].dt.year - cohort['anchor_year'] + cohort['anchor_age']\n",
    "cohort = cohort[cohort['age'] >= MIN_AGE]\n",
    "print(f\"  Adults (≥{MIN_AGE}): {len(cohort):,}\")\n",
    "\n",
    "# Survival variables\n",
    "cohort['dischtime'] = pd.to_datetime(cohort['dischtime'])\n",
    "cohort['los_days'] = (cohort['dischtime'] - cohort['admittime']).dt.total_seconds() / (24 * 3600)\n",
    "cohort['Time'] = cohort['los_days']\n",
    "cohort['Event'] = cohort['hospital_expire_flag']\n",
    "\n",
    "print(f\"  Deaths: {cohort['Event'].sum():,} ({cohort['Event'].mean()*100:.1f}%)\")\n",
    "\n",
    "# Select columns\n",
    "cohort_base = cohort[[\n",
    "    'subject_id', 'hadm_id', 'age', 'gender', \n",
    "    'admittime', 'dischtime', 'Time', 'Event'\n",
    "]].copy()\n",
    "\n",
    "cohort_base = cohort_base.rename(columns={'age': 'Age', 'gender': 'Sex'})\n",
    "\n",
    "# Save\n",
    "cohort_file = os.path.join(INTERMEDIATE_PATH, 'cohort_base.csv')\n",
    "cohort_base.to_csv(cohort_file, index=False)\n",
    "print(f\"\\nSaved: {cohort_file}\")\n",
    "print(f\"   Cohort: {len(cohort_base):,} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96204c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                 STEP 2: EXTRACTING LABORATORY VALUES                 \n",
      "======================================================================\n",
      "\n",
      "1. Extracting lab values (this may take 5-10 minutes)...\n",
      "  Processing chunk 155...\n",
      "  Extracted 59,253 lab measurements\n",
      "\n",
      "2. Lab coverage:\n",
      "  ALT: 366 (43.0%)\n",
      "  AST: 373 (43.8%)\n",
      "  Lac: 453 (53.2%)\n",
      "  Na: 790 (92.7%)\n",
      "  Cl: 790 (92.7%)\n",
      "  Plt: 796 (93.4%)\n",
      "\n",
      "Saved: /Users/azmannads/Downloads/Dr. Zhang/intermediate/cohort_with_labs.csv\n"
     ]
    }
   ],
   "source": [
    "##Extract labs\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: EXTRACT LABS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: EXTRACTING LABORATORY VALUES\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load cohort\n",
    "cohort = pd.read_csv(os.path.join(INTERMEDIATE_PATH, 'cohort_base.csv'))\n",
    "hadm_ids = cohort['hadm_id'].values\n",
    "\n",
    "# Get all lab item IDs\n",
    "all_lab_ids = [item for items in LAB_ITEMS.values() for item in items]\n",
    "item_to_name = {}\n",
    "for name, ids in LAB_ITEMS.items():\n",
    "    for item_id in ids:\n",
    "        item_to_name[item_id] = name\n",
    "\n",
    "print(\"\\n1. Extracting lab values (this may take 5-10 minutes)...\")\n",
    "labs_list = []\n",
    "chunk_count = 0\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    f'{MIMIC_PATH}hosp/labevents.csv.gz',\n",
    "    compression='gzip',\n",
    "    chunksize=1000000,\n",
    "    usecols=['hadm_id', 'itemid', 'valuenum']\n",
    "):\n",
    "    chunk_count += 1\n",
    "    if chunk_count % 5 == 0:\n",
    "        print(f\"  Processing chunk {chunk_count}...\", end='\\r')\n",
    "    \n",
    "    chunk_filtered = chunk[\n",
    "        (chunk['hadm_id'].isin(hadm_ids)) &\n",
    "        (chunk['itemid'].isin(all_lab_ids)) &\n",
    "        (chunk['valuenum'].notna())\n",
    "    ]\n",
    "    \n",
    "    if len(chunk_filtered) > 0:\n",
    "        labs_list.append(chunk_filtered)\n",
    "\n",
    "print()\n",
    "\n",
    "if labs_list:\n",
    "    labs = pd.concat(labs_list, ignore_index=True)\n",
    "    print(f\"  Extracted {len(labs):,} lab measurements\")\n",
    "    \n",
    "    # Pivot\n",
    "    labs['lab_name'] = labs['itemid'].map(item_to_name)\n",
    "    labs_first = labs.groupby(['hadm_id', 'lab_name'])['valuenum'].first().reset_index()\n",
    "    labs_wide = labs_first.pivot(\n",
    "        index='hadm_id',\n",
    "        columns='lab_name',\n",
    "        values='valuenum'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge\n",
    "    cohort = cohort.merge(labs_wide, on='hadm_id', how='left')\n",
    "    \n",
    "    # Rename\n",
    "    cohort = cohort.rename(columns={\n",
    "        'Lactate': 'Lac',\n",
    "        'Sodium': 'Na',\n",
    "        'Chloride': 'Cl',\n",
    "        'Platelets': 'Plt'\n",
    "    })\n",
    "    \n",
    "    # Coverage\n",
    "    print(\"\\n2. Lab coverage:\")\n",
    "    for lab in ['ALT', 'AST', 'Lac', 'Na', 'Cl', 'Plt']:\n",
    "        if lab in cohort.columns:\n",
    "            pct = cohort[lab].notna().sum() / len(cohort) * 100\n",
    "            print(f\"  {lab}: {cohort[lab].notna().sum():,} ({pct:.1f}%)\")\n",
    "\n",
    "# Save\n",
    "labs_file = os.path.join(INTERMEDIATE_PATH, 'cohort_with_labs.csv')\n",
    "cohort.to_csv(labs_file, index=False)\n",
    "print(f\"\\nSaved: {labs_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fea23fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    STEP 3: EXTRACTING VITAL SIGNS                    \n",
      "======================================================================\n",
      "\n",
      "1. Loading ICU stays...\n",
      "  ICU stays: 501\n",
      "\n",
      "2. Extracting vitals (this may take 10-15 minutes)...\n",
      "  Processing chunk 430...\n",
      "  Extracted 235,798 vital measurements\n",
      "\n",
      "3. Vital coverage:\n",
      "  HR: 378 (44.4%)\n",
      "  RR: 378 (44.4%)\n",
      "\n",
      "Saved: /Users/azmannads/Downloads/Dr. Zhang/intermediate/cohort_with_vitals.csv\n"
     ]
    }
   ],
   "source": [
    "## Extract vitals\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: EXTRACT VITALS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: EXTRACTING VITAL SIGNS\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load cohort\n",
    "cohort = pd.read_csv(os.path.join(INTERMEDIATE_PATH, 'cohort_with_labs.csv'))\n",
    "\n",
    "# Load ICU stays\n",
    "print(\"\\n1. Loading ICU stays...\")\n",
    "icustays = pd.read_csv(\n",
    "    f'{MIMIC_PATH}icu/icustays.csv.gz',\n",
    "    compression='gzip',\n",
    "    usecols=['subject_id', 'hadm_id', 'stay_id']\n",
    ")\n",
    "\n",
    "cohort_icu = cohort.merge(icustays, on=['subject_id', 'hadm_id'], how='left')\n",
    "stay_ids = cohort_icu['stay_id'].dropna().unique()\n",
    "print(f\"  ICU stays: {len(stay_ids):,}\")\n",
    "\n",
    "if len(stay_ids) > 0:\n",
    "    # Get vital item IDs\n",
    "    all_vital_ids = [item for items in VITAL_ITEMS.values() for item in items]\n",
    "    item_to_name = {}\n",
    "    for name, ids in VITAL_ITEMS.items():\n",
    "        for item_id in ids:\n",
    "            item_to_name[item_id] = name\n",
    "    \n",
    "    print(\"\\n2. Extracting vitals (this may take 10-15 minutes)...\")\n",
    "    vitals_list = []\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        f'{MIMIC_PATH}icu/chartevents.csv.gz',\n",
    "        compression='gzip',\n",
    "        chunksize=1000000,\n",
    "        usecols=['stay_id', 'itemid', 'valuenum']\n",
    "    ):\n",
    "        chunk_count += 1\n",
    "        if chunk_count % 10 == 0:\n",
    "            print(f\"  Processing chunk {chunk_count}...\", end='\\r')\n",
    "        \n",
    "        chunk_filtered = chunk[\n",
    "            (chunk['stay_id'].isin(stay_ids)) &\n",
    "            (chunk['itemid'].isin(all_vital_ids)) &\n",
    "            (chunk['valuenum'].notna())\n",
    "        ]\n",
    "        \n",
    "        if len(chunk_filtered) > 0:\n",
    "            vitals_list.append(chunk_filtered)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    if vitals_list:\n",
    "        vitals = pd.concat(vitals_list, ignore_index=True)\n",
    "        print(f\"  Extracted {len(vitals):,} vital measurements\")\n",
    "        \n",
    "        # Pivot\n",
    "        vitals['vital_name'] = vitals['itemid'].map(item_to_name)\n",
    "        vitals_mean = vitals.groupby(['stay_id', 'vital_name'])['valuenum'].mean().reset_index()\n",
    "        vitals_wide = vitals_mean.pivot(\n",
    "            index='stay_id',\n",
    "            columns='vital_name',\n",
    "            values='valuenum'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Merge\n",
    "        cohort_icu = cohort_icu.merge(vitals_wide, on='stay_id', how='left')\n",
    "        vital_cols = [c for c in ['HR', 'RR'] if c in cohort_icu.columns]\n",
    "        \n",
    "        cohort = cohort.merge(\n",
    "            cohort_icu[['hadm_id'] + vital_cols].drop_duplicates('hadm_id'),\n",
    "            on='hadm_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Coverage\n",
    "        print(\"\\n3. Vital coverage:\")\n",
    "        for vital in ['HR', 'RR']:\n",
    "            if vital in cohort.columns:\n",
    "                pct = cohort[vital].notna().sum() / len(cohort) * 100\n",
    "                print(f\"  {vital}: {cohort[vital].notna().sum():,} ({pct:.1f}%)\")\n",
    "\n",
    "# Save\n",
    "vitals_file = os.path.join(INTERMEDIATE_PATH, 'cohort_with_vitals.csv')\n",
    "cohort.to_csv(vitals_file, index=False)\n",
    "print(f\"\\nSaved: {vitals_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "966b512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                    STEP 4: EXTRACTING GCS SCORES                     \n",
      "======================================================================\n",
      "\n",
      "1. Extracting GCS (this may take 10-15 minutes)...\n",
      "  Processing chunk 430...\n",
      "  Extracted 72,326 GCS measurements\n",
      "  GCS: 375 (44.0%)\n",
      "\n",
      "Saved: /Users/azmannads/Downloads/Dr. Zhang/intermediate/cohort_with_gcs.csv\n"
     ]
    }
   ],
   "source": [
    "## Extract GCS\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: EXTRACT GCS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: EXTRACTING GCS SCORES\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load cohort\n",
    "cohort = pd.read_csv(os.path.join(INTERMEDIATE_PATH, 'cohort_with_vitals.csv'))\n",
    "\n",
    "# ICU stays\n",
    "cohort_icu = cohort.merge(icustays, on=['subject_id', 'hadm_id'], how='left')\n",
    "stay_ids = cohort_icu['stay_id'].dropna().unique()\n",
    "\n",
    "if len(stay_ids) > 0:\n",
    "    print(\"\\n1. Extracting GCS (this may take 10-15 minutes)...\")\n",
    "    gcs_list = []\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        f'{MIMIC_PATH}icu/chartevents.csv.gz',\n",
    "        compression='gzip',\n",
    "        chunksize=1000000,\n",
    "        usecols=['stay_id', 'itemid', 'valuenum']\n",
    "    ):\n",
    "        chunk_count += 1\n",
    "        if chunk_count % 10 == 0:\n",
    "            print(f\"  Processing chunk {chunk_count}...\", end='\\r')\n",
    "        \n",
    "        chunk_filtered = chunk[\n",
    "            (chunk['stay_id'].isin(stay_ids)) &\n",
    "            (chunk['itemid'].isin(GCS_ITEMS)) &\n",
    "            (chunk['valuenum'].notna())\n",
    "        ]\n",
    "        \n",
    "        if len(chunk_filtered) > 0:\n",
    "            gcs_list.append(chunk_filtered)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    if gcs_list:\n",
    "        gcs = pd.concat(gcs_list, ignore_index=True)\n",
    "        print(f\"  Extracted {len(gcs):,} GCS measurements\")\n",
    "        \n",
    "        # Get total GCS\n",
    "        gcs_total = gcs[gcs['itemid'] == 198].groupby('stay_id')['valuenum'].min()\n",
    "        \n",
    "        if len(gcs_total) == 0:\n",
    "            gcs['component'] = gcs['itemid'].map({\n",
    "                220739: 'eye',\n",
    "                223901: 'motor',\n",
    "                223900: 'verbal'\n",
    "            })\n",
    "            gcs_comp = gcs[gcs['component'].notna()].pivot_table(\n",
    "                index='stay_id',\n",
    "                columns='component',\n",
    "                values='valuenum',\n",
    "                aggfunc='min'\n",
    "            )\n",
    "            if len(gcs_comp) > 0:\n",
    "                gcs_total = gcs_comp.sum(axis=1)\n",
    "        \n",
    "        if len(gcs_total) > 0:\n",
    "            gcs_scores = gcs_total.reset_index().rename(columns={0: 'GCS'})\n",
    "            cohort_icu = cohort_icu.merge(gcs_scores, on='stay_id', how='left')\n",
    "            \n",
    "            cohort = cohort.merge(\n",
    "                cohort_icu[['hadm_id', 'GCS']].drop_duplicates('hadm_id'),\n",
    "                on='hadm_id',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            pct = cohort['GCS'].notna().sum() / len(cohort) * 100\n",
    "            print(f\"  GCS: {cohort['GCS'].notna().sum():,} ({pct:.1f}%)\")\n",
    "\n",
    "# Save\n",
    "gcs_file = os.path.join(INTERMEDIATE_PATH, 'cohort_with_gcs.csv')\n",
    "cohort.to_csv(gcs_file, index=False)\n",
    "print(f\"\\nSaved: {gcs_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21648f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                   STEP 5: CLEANING AND FINALIZING                    \n",
      "======================================================================\n",
      "\n",
      "1. Starting with: 852 patients\n",
      "\n",
      "2. Selected 15 analysis variables\n",
      "\n",
      "3. Removing invalid survival times...\n",
      "\n",
      "4. Final cohort: 852 patients\n",
      "   Events (deaths): 59 (6.9%)\n",
      "   Censored: 793 (93.1%)\n",
      "\n",
      "Saved complete dataset: /Users/azmannads/Downloads/Dr. Zhang/final/mimic_sepsis_cohort_full.csv\n",
      "\n",
      "======================================================================\n",
      "                      DATA EXTRACTION COMPLETED!                      \n",
      "======================================================================\n",
      "\n",
      "You can now handle missing data and train/test splitting\n",
      "according to your own methods.\n"
     ]
    }
   ],
   "source": [
    "## Clean and finalize\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: CLEAN AND FINALIZE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: CLEANING AND FINALIZING\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load cohort\n",
    "cohort = pd.read_csv(os.path.join(INTERMEDIATE_PATH, 'cohort_with_gcs.csv'))\n",
    "\n",
    "print(f\"\\n1. Starting with: {len(cohort):,} patients\")\n",
    "\n",
    "# Select analysis columns\n",
    "analysis_cols = [\n",
    "    'subject_id', 'hadm_id', 'Age', 'Sex',\n",
    "    'ALT', 'AST', 'Lac', 'Na', 'Cl', 'Plt',\n",
    "    'HR', 'RR', 'GCS',\n",
    "    'Time', 'Event'\n",
    "]\n",
    "\n",
    "available_cols = [col for col in analysis_cols if col in cohort.columns]\n",
    "cohort_clean = cohort[available_cols].copy()\n",
    "\n",
    "print(f\"\\n2. Selected {len(available_cols)} analysis variables\")\n",
    "\n",
    "# Convert Sex to numeric\n",
    "cohort_clean['Sex'] = cohort_clean['Sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "# Remove invalid times (negative or zero survival times)\n",
    "print(\"\\n3. Removing invalid survival times...\")\n",
    "before = len(cohort_clean)\n",
    "cohort_clean = cohort_clean[cohort_clean['Time'] > 0]\n",
    "removed = before - len(cohort_clean)\n",
    "if removed > 0:\n",
    "    print(f\"  Removed {removed} patients with invalid survival times\")\n",
    "\n",
    "print(f\"\\n4. Final cohort: {len(cohort_clean):,} patients\")\n",
    "print(f\"   Events (deaths): {cohort_clean['Event'].sum():,} ({cohort_clean['Event'].mean()*100:.1f}%)\")\n",
    "print(f\"   Censored: {(cohort_clean['Event']==0).sum():,} ({(cohort_clean['Event']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Save full dataset\n",
    "full_file = os.path.join(FINAL_PATH, 'mimic_sepsis_cohort_full.csv')\n",
    "cohort_clean.to_csv(full_file, index=False)\n",
    "print(f\"\\nSaved complete dataset: {full_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA EXTRACTION COMPLETED!\".center(70))\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou can now handle missing data and train/test splitting\")\n",
    "print(\"according to your own methods.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a18bdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                           DATASET SUMMARY                            \n",
      "======================================================================\n",
      "\n",
      "Cohort Overview:\n",
      "  Total patients: 852\n",
      "  Events (deaths): 59 (6.9%)\n",
      "  Censored: 793 (93.1%)\n",
      "\n",
      "======================================================================\n",
      "VARIABLE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Age:\n",
      "  Available: 852/852 (100.0%)\n",
      "  Missing:   0 (0.0%)\n",
      "  Mean ± SD: 60.23 ± 14.86\n",
      "  Median:    62.00\n",
      "  Range:     [19.00, 91.00]\n",
      "\n",
      "Sex:\n",
      "  Available: 852/852 (100.0%)\n",
      "  Missing:   0 (0.0%)\n",
      "  Mean ± SD: 0.50 ± 0.50\n",
      "  Median:    1.00\n",
      "  Range:     [0.00, 1.00]\n",
      "\n",
      "ALT:\n",
      "  Available: 366/852 (43.0%)\n",
      "  Missing:   486 (57.0%)\n",
      "  Mean ± SD: 144.22 ± 443.02\n",
      "  Median:    25.50\n",
      "  Range:     [1.00, 4061.00]\n",
      "\n",
      "AST:\n",
      "  Available: 373/852 (43.8%)\n",
      "  Missing:   479 (56.2%)\n",
      "  Mean ± SD: 194.40 ± 609.10\n",
      "  Median:    39.00\n",
      "  Range:     [6.00, 6000.00]\n",
      "\n",
      "HR:\n",
      "  Available: 378/852 (44.4%)\n",
      "  Missing:   474 (55.6%)\n",
      "  Mean ± SD: 91.03 ± 12.59\n",
      "  Median:    90.03\n",
      "  Range:     [59.11, 143.93]\n",
      "\n",
      "RR:\n",
      "  Available: 378/852 (44.4%)\n",
      "  Missing:   474 (55.6%)\n",
      "  Mean ± SD: 20.28 ± 3.77\n",
      "  Median:    19.79\n",
      "  Range:     [12.54, 32.04]\n",
      "\n",
      "GCS:\n",
      "  Available: 375/852 (44.0%)\n",
      "  Missing:   477 (56.0%)\n",
      "  Mean ± SD: 7.81 ± 4.89\n",
      "  Median:    7.00\n",
      "  Range:     [3.00, 15.00]\n",
      "\n",
      "Lac:\n",
      "  Available: 453/852 (53.2%)\n",
      "  Missing:   399 (46.8%)\n",
      "  Mean ± SD: 1.99 ± 1.53\n",
      "  Median:    1.60\n",
      "  Range:     [0.40, 13.40]\n",
      "\n",
      "Na:\n",
      "  Available: 790/852 (92.7%)\n",
      "  Missing:   62 (7.3%)\n",
      "  Mean ± SD: 137.95 ± 3.14\n",
      "  Median:    138.00\n",
      "  Range:     [127.00, 154.00]\n",
      "\n",
      "Cl:\n",
      "  Available: 790/852 (92.7%)\n",
      "  Missing:   62 (7.3%)\n",
      "  Mean ± SD: 103.49 ± 4.00\n",
      "  Median:    104.00\n",
      "  Range:     [92.00, 124.00]\n",
      "\n",
      "Plt:\n",
      "  Available: 796/852 (93.4%)\n",
      "  Missing:   56 (6.6%)\n",
      "  Mean ± SD: 232.26 ± 101.89\n",
      "  Median:    218.00\n",
      "  Range:     [16.00, 834.00]\n",
      "\n",
      "Time:\n",
      "  Available: 852/852 (100.0%)\n",
      "  Missing:   0 (0.0%)\n",
      "  Mean ± SD: 13.04 ± 17.76\n",
      "  Median:    7.00\n",
      "  Range:     [0.14, 220.63]\n",
      "\n",
      "======================================================================\n",
      "MISSING DATA SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Variables with missing data:\n",
      "Variable  Missing  Missing %\n",
      "     ALT      486       57.0\n",
      "     AST      479       56.2\n",
      "     GCS      477       56.0\n",
      "      HR      474       55.6\n",
      "      RR      474       55.6\n",
      "     Lac      399       46.8\n",
      "      Na       62        7.3\n",
      "      Cl       62        7.3\n",
      "     Plt       56        6.6\n",
      "\n",
      "======================================================================\n",
      "FILES SAVED\n",
      "======================================================================\n",
      "\n",
      "📁 Location: /Users/azmannads/Downloads/Dr. Zhang/final\n",
      "\n",
      "mimic_sepsis_cohort_full.csv\n",
      "   852 patients × 15 variables\n",
      "\n",
      "======================================================================\n",
      "NEXT STEPS\n",
      "======================================================================\n",
      "\n",
      "1. Handle missing data (imputation, deletion, etc.)\n",
      "2. Create training/validation/test splits\n",
      "3. Run your RSF analysis\n",
      "4. Compare with Cox PH and Kaplan-Meier\n",
      "5. Validate on test set\n",
      "\n",
      "======================================================================\n",
      "                       Ready for your analysis!                       \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "## Final summary and data quality report\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY & DATA QUALITY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET SUMMARY\".center(70))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nCohort Overview:\")\n",
    "print(f\"  Total patients: {len(cohort_clean):,}\")\n",
    "print(f\"  Events (deaths): {cohort_clean['Event'].sum():,} ({cohort_clean['Event'].mean()*100:.1f}%)\")\n",
    "print(f\"  Censored: {(cohort_clean['Event']==0).sum():,} ({(cohort_clean['Event']==0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VARIABLE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_vars = ['Age', 'Sex', 'ALT', 'AST', 'HR', 'RR', 'GCS', 'Lac', 'Na', 'Cl', 'Plt', 'Time']\n",
    "\n",
    "for var in summary_vars:\n",
    "    if var in cohort_clean.columns:\n",
    "        n_available = cohort_clean[var].notna().sum()\n",
    "        n_missing = cohort_clean[var].isna().sum()\n",
    "        pct_available = (n_available / len(cohort_clean)) * 100\n",
    "        \n",
    "        if n_available > 0:\n",
    "            mean_val = cohort_clean[var].mean()\n",
    "            std_val = cohort_clean[var].std()\n",
    "            median_val = cohort_clean[var].median()\n",
    "            min_val = cohort_clean[var].min()\n",
    "            max_val = cohort_clean[var].max()\n",
    "            \n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  Available: {n_available:,}/{len(cohort_clean):,} ({pct_available:.1f}%)\")\n",
    "            print(f\"  Missing:   {n_missing:,} ({(n_missing/len(cohort_clean))*100:.1f}%)\")\n",
    "            print(f\"  Mean ± SD: {mean_val:.2f} ± {std_val:.2f}\")\n",
    "            print(f\"  Median:    {median_val:.2f}\")\n",
    "            print(f\"  Range:     [{min_val:.2f}, {max_val:.2f}]\")\n",
    "        else:\n",
    "            print(f\"\\n{var}:\")\n",
    "            print(f\"  ⚠️  ALL VALUES MISSING\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MISSING DATA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Variable': cohort_clean.columns,\n",
    "    'Missing': cohort_clean.isnull().sum(),\n",
    "    'Missing %': (cohort_clean.isnull().sum() / len(cohort_clean) * 100).round(1)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing'] > 0].sort_values('Missing', ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"\\nVariables with missing data:\")\n",
    "    print(missing_summary.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo missing data!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILES SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📁 Location: {FINAL_PATH}\")\n",
    "print(f\"\\nmimic_sepsis_cohort_full.csv\")\n",
    "print(f\"   {len(cohort_clean):,} patients × {len(cohort_clean.columns)} variables\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Handle missing data (imputation, deletion, etc.)\")\n",
    "print(\"2. Create training/validation/test splits\")\n",
    "print(\"3. Run your RSF analysis\")\n",
    "print(\"4. Compare with Cox PH and Kaplan-Meier\")\n",
    "print(\"5. Validate on test set\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Ready for your analysis!\".center(70))\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
